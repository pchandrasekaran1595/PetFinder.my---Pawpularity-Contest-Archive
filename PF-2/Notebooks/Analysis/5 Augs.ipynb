{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PF - 5 Augs",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYG5LtYLZPTy"
      },
      "source": [
        "## Get Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eGFdAnwzY3SY"
      },
      "source": [
        "from IPython.display import clear_output\n",
        "from google.colab import files\n",
        "files.upload()\n",
        "\n",
        "!pip install -q kaggle\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!ls ~/.kaggle\n",
        "!chmod 600 /root/.kaggle/kaggle.json\n",
        "\n",
        "!kaggle datasets download -d jackstapleton/petfinder-pf-cc-ua-all-dataset\n",
        "!kaggle datasets download -d jackstapleton/petfinder-pf-d169-nc-a-dataset\n",
        "\n",
        "!mkdir ~/.data_ua\n",
        "!mkdir ~/.data_a\n",
        "\n",
        "!unzip -q petfinder-pf-cc-ua-all-dataset.zip -d /.data_ua\n",
        "!unzip -q petfinder-pf-d169-nc-a-dataset.zip -d /.data_a\n",
        "\n",
        "clear_output()"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "peelElqfTNbw",
        "outputId": "950e5c00-d1a7-49eb-d346-aaa410c3ba94"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkzgE5ztTRwK"
      },
      "source": [
        "import os\n",
        "import re\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader as DL\n",
        "from torch.nn.utils import weight_norm as WN\n",
        "from torchvision import models, transforms\n",
        "\n",
        "from time import time\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OgrroiOcURXq"
      },
      "source": [
        "SEED = 49\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "PATH_UA = \"/.data_ua\"\n",
        "PATH_A = \"/.data_a\"\n",
        "\n",
        "verbose = False\n",
        "DEBUG = False\n",
        "\n",
        "names = sorted(os.listdir(PATH_A))[:5]\n",
        "\n",
        "sc_y = StandardScaler()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1hy01DrUSx6"
      },
      "source": [
        "def breaker(num=50, char=\"*\") -> None:\n",
        "    print(\"\\n\" + num*char + \"\\n\")\n",
        "\n",
        "\n",
        "def get_targets() -> np.ndarray:\n",
        "    df = pd.read_csv(\"/content/gdrive/My Drive/train.csv\", engine=\"python\")\n",
        "    targets = df[\"Pawpularity\"].copy().values\n",
        "    return targets.reshape(-1, 1)\n",
        "\n",
        "\n",
        "def show_graphs(L: list, title=None) -> None:\n",
        "    TL, VL = [], []\n",
        "    for i in range(len(L)):\n",
        "        TL.append(L[i][\"train\"])\n",
        "        VL.append(L[i][\"valid\"])\n",
        "    x_Axis = np.arange(1, len(L) + 1)\n",
        "    plt.figure()\n",
        "    plt.plot(x_Axis, TL, \"r\", label=\"train\")\n",
        "    plt.plot(x_Axis, VL, \"b\", label=\"valid\")\n",
        "    plt.grid()\n",
        "    plt.legend()\n",
        "    if title:\n",
        "        plt.title(\"{} Loss\".format(title))\n",
        "    else:\n",
        "        plt.title(\"Loss\")\n",
        "    plt.show()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFX0TudfZIPa"
      },
      "source": [
        "## Independent and Sum Losses"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "celyafJGZOVl"
      },
      "source": [
        "def do_independent(features_1: np.ndarray, features_2: np.ndarray, features_3: np.ndarray, \n",
        "                   features_4: np.ndarray, features_5: np.ndarray, features_6: np.ndarray,\n",
        "                   targets: np.ndarray):\n",
        "\n",
        "    class DS(Dataset):\n",
        "        def __init__(self, features_1, features_2, features_3, features_4, features_5, features_6, targets):\n",
        "            self.features_1 = features_1\n",
        "            self.features_2 = features_2\n",
        "            self.features_3 = features_3\n",
        "            self.features_4 = features_4\n",
        "            self.features_5 = features_5\n",
        "            self.features_6 = features_6\n",
        "            self.targets = targets\n",
        "\n",
        "        def __len__(self):\n",
        "            return self.features_1.shape[0]\n",
        "        \n",
        "        def __getitem__(self, idx):\n",
        "            return torch.FloatTensor(self.features_1[idx]), \\\n",
        "                torch.FloatTensor(self.features_2[idx]), \\\n",
        "                torch.FloatTensor(self.features_3[idx]), \\\n",
        "                torch.FloatTensor(self.features_4[idx]), \\\n",
        "                torch.FloatTensor(self.features_5[idx]), \\\n",
        "                torch.FloatTensor(self.features_6[idx]), \\\n",
        "                torch.FloatTensor(self.targets[idx])\n",
        "\n",
        "\n",
        "    def build_dataloaders(tr_features_1: np.ndarray, tr_features_2: np.ndarray, tr_features_3: np.ndarray, \n",
        "                          tr_features_4: np.ndarray, tr_features_5: np.ndarray, tr_features_6: np.ndarray,\n",
        "                          va_features_1: np.ndarray, va_features_2: np.ndarray, va_features_3: np.ndarray, \n",
        "                          va_features_4: np.ndarray, va_features_5: np.ndarray, va_features_6: np.ndarray,\n",
        "                          tr_targets: np.ndarray, va_targets: np.ndarray, batch_size: int, seed: int):\n",
        "\n",
        "        if verbose:\n",
        "            breaker()\n",
        "            print(\"Building Train and Valid Dataloaders ...\")\n",
        "\n",
        "        tr_data_setup = DS(tr_features_1, tr_features_2, tr_features_3, tr_features_4, tr_features_5, tr_features_6, tr_targets)\n",
        "        va_data_setup = DS(va_features_1, va_features_2, va_features_3, va_features_4, va_features_5, va_features_6, va_targets)\n",
        "\n",
        "        dataloaders = {\n",
        "            \"train\" : DL(tr_data_setup, batch_size=batch_size, shuffle=True, generator=torch.manual_seed(seed)),\n",
        "            \"valid\" : DL(va_data_setup, batch_size=batch_size, shuffle=False)\n",
        "        }\n",
        "\n",
        "        return dataloaders\n",
        "\n",
        "\n",
        "    def build_model(IL: int, seed: int):\n",
        "        class ANN(nn.Module):\n",
        "            def __init__(self, IL=None):\n",
        "                super(ANN, self).__init__()\n",
        "\n",
        "                self.predictor = nn.Sequential()\n",
        "                self.predictor.add_module(\"BN\", nn.BatchNorm1d(num_features=IL, eps=1e-5))\n",
        "                self.predictor.add_module(\"FC\", WN(nn.Linear(in_features=IL, out_features=1)))\n",
        "\n",
        "            def get_optimizer(self, lr=1e-3, wd=0):\n",
        "                params = [p for p in self.parameters() if p.requires_grad]\n",
        "                return optim.Adam(params, lr=lr, weight_decay=wd)\n",
        "\n",
        "            def get_plateau_scheduler(self, optimizer=None, patience=5, eps=1e-8):\n",
        "                return optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, patience=patience, eps=eps, verbose=True)\n",
        "\n",
        "            def forward(self, x1, x2=None, x3=None, x4=None, x5=None, x6=None):\n",
        "                if x2 is not None and x3 is not None and x4 is not None and x5 is not None and x6 is not None:\n",
        "                    return self.predictor(x1), self.predictor(x2), self.predictor(x3), self.predictor(x4), self.predictor(x5), self.predictor(x6)\n",
        "                else:\n",
        "                    return self.predictor(x1)\n",
        "        \n",
        "        if verbose:\n",
        "            breaker()\n",
        "            print(\"Building Model ...\")\n",
        "            print(\"\\n{} -> 1\".format(IL))\n",
        "        \n",
        "        torch.manual_seed(seed)\n",
        "        model = ANN(IL=IL)\n",
        "        \n",
        "        return model\n",
        "\n",
        "\n",
        "    def fit(model=None, optimizer=None, scheduler=None, \n",
        "            epochs=None, early_stopping_patience=None,\n",
        "            dataloaders=None, fold=None, lr=None, wd=None, verbose=False) -> tuple:\n",
        "        \n",
        "        name = \"./Independent_Fold_{}_state.pt\".format(lr, wd, fold)\n",
        "        \n",
        "        if verbose:\n",
        "            breaker()\n",
        "            print(\"Training Fold {}...\".format(fold))\n",
        "            breaker()\n",
        "        else:\n",
        "            print(\"Training Fold {}...\".format(fold))\n",
        "\n",
        "        Losses = []\n",
        "        bestLoss = {\"train\" : np.inf, \"valid\" : np.inf}\n",
        "\n",
        "        start_time = time()\n",
        "        for e in range(epochs):\n",
        "            e_st = time()\n",
        "            epochLoss = {\"train\" : np.inf, \"valid\" : np.inf}\n",
        "\n",
        "            for phase in [\"train\", \"valid\"]:\n",
        "                if phase == \"train\":\n",
        "                    model.train()\n",
        "                else:\n",
        "                    model.eval()\n",
        "                \n",
        "                lossPerPass = []\n",
        "\n",
        "                for X1, X2, X3, X4, X5, X6, y in dataloaders[phase]:\n",
        "                    X1, X2, X3, X4, X5, X6, y = X1.to(DEVICE), X2.to(DEVICE), X3.to(DEVICE), \\\n",
        "                                                X4.to(DEVICE), X5.to(DEVICE), X6.to(DEVICE), \\\n",
        "                                                y.to(DEVICE)\n",
        "\n",
        "                    optimizer.zero_grad()\n",
        "                    with torch.set_grad_enabled(phase == \"train\"):\n",
        "                        output_1, output_2, output_3, output_4, output_5, output_6 = model(X1, X2, X3, X4, X5, X6)\n",
        "                        loss = torch.nn.MSELoss()(output_1, y) + torch.nn.MSELoss()(output_2, y) + torch.nn.MSELoss()(output_3, y) + \\\n",
        "                               torch.nn.MSELoss()(output_4, y) + torch.nn.MSELoss()(output_5, y) + torch.nn.MSELoss()(output_6, y)\n",
        "                        if phase == \"train\":\n",
        "                            loss.backward()\n",
        "                            optimizer.step()\n",
        "                    lossPerPass.append(loss.item())\n",
        "                epochLoss[phase] = np.mean(np.array(lossPerPass))\n",
        "            Losses.append(epochLoss)\n",
        "\n",
        "            if early_stopping_patience:\n",
        "                if epochLoss[\"valid\"] < bestLoss[\"valid\"]:\n",
        "                    bestLoss = epochLoss\n",
        "                    BLE = e + 1\n",
        "                    torch.save({\"model_state_dict\": model.state_dict(),\n",
        "                                \"optim_state_dict\": optimizer.state_dict()},\n",
        "                            name)\n",
        "                    early_stopping_step = 0\n",
        "                else:\n",
        "                    early_stopping_step += 1\n",
        "                    if early_stopping_step > early_stopping_patience:\n",
        "                        if verbose:\n",
        "                            print(\"\\nEarly Stopping at Epoch {}\".format(e))\n",
        "                        break\n",
        "            \n",
        "            if epochLoss[\"valid\"] < bestLoss[\"valid\"]:\n",
        "                bestLoss = epochLoss\n",
        "                BLE = e + 1\n",
        "                torch.save({\"model_state_dict\": model.state_dict(),\n",
        "                            \"optim_state_dict\": optimizer.state_dict()},\n",
        "                        name)\n",
        "            \n",
        "            if scheduler:\n",
        "                scheduler.step(epochLoss[\"valid\"])\n",
        "            \n",
        "            if verbose:\n",
        "                print(\"Epoch: {} | Train Loss: {:.5f} | Valid Loss: {:.5f} | Time: {:.2f} seconds\".format(e+1, epochLoss[\"train\"], epochLoss[\"valid\"], time()-e_st))\n",
        "        \n",
        "        if verbose:\n",
        "            breaker()\n",
        "            print(\"Best Validation Loss at Epoch {}\".format(BLE))\n",
        "            breaker()\n",
        "            print(\"Time Taken [{} Epochs] : {:.2f} minutes\".format(len(Losses), (time()-start_time)/60))\n",
        "            breaker()\n",
        "            print(\"Training Completed\")\n",
        "            breaker()\n",
        "\n",
        "        return Losses, BLE, name\n",
        "\n",
        "    #####################################################################################################\n",
        "\n",
        "    def predict_batch(model=None, dataloader=None, mode=\"test\", path=None) -> np.ndarray:    \n",
        "        model.load_state_dict(torch.load(path, map_location=DEVICE)[\"model_state_dict\"])\n",
        "        model.to(DEVICE)\n",
        "        model.eval()\n",
        "\n",
        "        y_pred = torch.zeros(1, 1).to(DEVICE)\n",
        "        if re.match(r\"valid\", mode, re.IGNORECASE):\n",
        "            for X, _, _, _, _, _, _ in dataloader:\n",
        "                X = X.to(DEVICE)\n",
        "                with torch.no_grad():\n",
        "                    output = model(X)\n",
        "                y_pred = torch.cat((y_pred, output.view(-1, 1)), dim=0)\n",
        "        elif re.match(r\"test\", mode, re.IGNORECASE):\n",
        "            for X in dataloader:\n",
        "                X = X.to(DEVICE)\n",
        "                with torch.no_grad():\n",
        "                    output = model(X)\n",
        "                y_pred = torch.cat((y_pred, output.view(-1, 1)), dim=0)\n",
        "        \n",
        "        return y_pred[1:].detach().cpu().numpy()\n",
        "\n",
        "\n",
        "    def train(features_1: np.ndarray, features_2: np.ndarray, features_3: np.ndarray,\n",
        "              features_4: np.ndarray, features_5: np.ndarray, features_6: np.ndarray,\n",
        "              targets: np.ndarray, n_splits: int, \n",
        "              batch_size: int, lr: float, wd: float, \n",
        "              epochs: int, early_stopping: int, \n",
        "              patience=None, eps=None):\n",
        "\n",
        "        metrics = []\n",
        "            \n",
        "        KFold_start_time = time()\n",
        "        if verbose:\n",
        "            breaker()\n",
        "            print(\"Performing {} Fold CV ...\".format(n_splits))\n",
        "        else:\n",
        "            breaker()\n",
        "        fold = 1\n",
        "        for tr_idx, va_idx in KFold(n_splits=n_splits, shuffle=True, random_state=SEED).split(features_1):\n",
        "\n",
        "            tr_features_1, va_features_1 = features_1[tr_idx], features_1[va_idx]\n",
        "            tr_features_2, va_features_2 = features_2[tr_idx], features_2[va_idx]\n",
        "            tr_features_3, va_features_3 = features_3[tr_idx], features_3[va_idx]\n",
        "            tr_features_4, va_features_4 = features_4[tr_idx], features_4[va_idx]\n",
        "            tr_features_5, va_features_5 = features_5[tr_idx], features_5[va_idx]\n",
        "            tr_features_6, va_features_6 = features_6[tr_idx], features_6[va_idx]\n",
        "\n",
        "            tr_targets, va_targets = targets[tr_idx], targets[va_idx]\n",
        "\n",
        "            tr_targets = sc_y.fit_transform(tr_targets)\n",
        "            va_targets = sc_y.transform(va_targets)\n",
        "\n",
        "            dataloaders = build_dataloaders(tr_features_1, tr_features_2, tr_features_3, tr_features_4, tr_features_5, tr_features_6,\n",
        "                                            va_features_1, va_features_2, va_features_3, va_features_4, va_features_5, va_features_6,\n",
        "                                            tr_targets, va_targets, \n",
        "                                            batch_size, SEED)\n",
        "            model = build_model(IL=tr_features_1.shape[1], seed=SEED).to(DEVICE)\n",
        "            optimizer = model.get_optimizer(lr=lr, wd=wd)\n",
        "            scheduler = None\n",
        "            if isinstance(patience, int) and isinstance(eps, float):\n",
        "                scheduler = model.get_plateau_scheduler(optimizer, patience, eps)\n",
        "\n",
        "            L, _, name = fit(model=model, optimizer=optimizer, scheduler=scheduler, \n",
        "                            epochs=epochs, early_stopping_patience=early_stopping,\n",
        "                            dataloaders=dataloaders, fold=fold, lr=lr, wd=wd, verbose=verbose)\n",
        "            y_pred = predict_batch(model=model, dataloader=dataloaders[\"valid\"], mode=\"valid\", path=name)\n",
        "            RMSE = np.sqrt(mean_squared_error(sc_y.inverse_transform(y_pred), sc_y.inverse_transform(va_targets)))\n",
        "            if verbose:\n",
        "                print(\"Validation RMSE [Fold {}]: {:.5f}\".format(fold, RMSE))\n",
        "                breaker()\n",
        "                show_graphs(L)\n",
        "            \n",
        "            metrics_dict = {\"Fold\" : fold, \"RMSE\" : RMSE}\n",
        "            metrics.append(metrics_dict)\n",
        "            \n",
        "            fold += 1\n",
        "        \n",
        "        if verbose:\n",
        "            breaker()\n",
        "            print(\"Total Time to {} Fold CV : {:.2f} minutes\".format(n_splits, (time() - KFold_start_time)/60))\n",
        "        \n",
        "        return metrics, (time() - KFold_start_time)/60\n",
        "    \n",
        "    metrics, _ = train(features_1, features_2, features_3, features_4, features_5, features_6, targets, 10, 128, 1e-3, 1e-1, 100, 8)\n",
        "\n",
        "    if verbose:\n",
        "        pass\n",
        "    else:\n",
        "        breaker()\n",
        "    rmse = []\n",
        "    for i in range(len(metrics)):\n",
        "        print(\"Fold {}, RMSE {:.5f}\".format(metrics[i][\"Fold\"], metrics[i][\"RMSE\"]))\n",
        "        rmse.append(metrics[i][\"RMSE\"])\n",
        "    breaker()\n",
        "\n",
        "    best_index = rmse.index(min(rmse))\n",
        "\n",
        "    print(\"Best RMSE : {:.5f}\".format(metrics[best_index][\"RMSE\"]))\n",
        "    print(\"Avg RMSE  : {:.5f}\".format(sum(rmse) / len(rmse)))\n",
        "\n",
        "    with open(\"metrics_1.pkl\", \"wb\") as fp:\n",
        "        pickle.dump(metrics, fp)\n",
        "\n",
        "    breaker() "
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ce6gDHHXoAR"
      },
      "source": [
        "**Main**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abL4hdhrZJmb"
      },
      "source": [
        "## Concate to Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gSs5uSOpZOwX"
      },
      "source": [
        "def do_concat(features: np.ndarray, targets: np.ndarray):\n",
        "\n",
        "    class DS(Dataset):\n",
        "        def __init__(self, features, targets):\n",
        "            self.features = features\n",
        "            self.targets = targets\n",
        "\n",
        "        def __len__(self):\n",
        "            return self.features.shape[0]\n",
        "        \n",
        "        def __getitem__(self, idx):\n",
        "            return torch.FloatTensor(self.features[idx]), torch.FloatTensor(self.targets[idx])\n",
        "\n",
        "\n",
        "    def build_dataloaders(tr_features: np.ndarray, va_features: np.ndarray,\n",
        "                          tr_targets: np.ndarray, va_targets: np.ndarray, \n",
        "                          batch_size: int, seed: int):\n",
        "\n",
        "        if verbose:\n",
        "            breaker()\n",
        "            print(\"Building Train and Valid Dataloaders ...\")\n",
        "\n",
        "        tr_data_setup = DS(tr_features, tr_targets)\n",
        "        va_data_setup = DS(va_features, va_targets)\n",
        "\n",
        "        dataloaders = {\n",
        "            \"train\" : DL(tr_data_setup, batch_size=batch_size, shuffle=True, generator=torch.manual_seed(seed)),\n",
        "            \"valid\" : DL(va_data_setup, batch_size=batch_size, shuffle=False)\n",
        "        }\n",
        "\n",
        "        return dataloaders\n",
        "\n",
        "\n",
        "    def build_model(IL: int, seed: int):\n",
        "        class ANN(nn.Module):\n",
        "            def __init__(self, IL=None):\n",
        "                super(ANN, self).__init__()\n",
        "\n",
        "                self.predictor = nn.Sequential()\n",
        "                self.predictor.add_module(\"BN\", nn.BatchNorm1d(num_features=IL, eps=1e-5))\n",
        "                self.predictor.add_module(\"FC\", WN(nn.Linear(in_features=IL, out_features=1)))\n",
        "\n",
        "            def get_optimizer(self, lr=1e-3, wd=0):\n",
        "                params = [p for p in self.parameters() if p.requires_grad]\n",
        "                return optim.Adam(params, lr=lr, weight_decay=wd)\n",
        "\n",
        "            def get_plateau_scheduler(self, optimizer=None, patience=5, eps=1e-8):\n",
        "                return optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, patience=patience, eps=eps, verbose=True)\n",
        "\n",
        "            def forward(self, x1, x2=None):\n",
        "                if x2 is not None:\n",
        "                    return self.predictor(x1), self.predictor(x2)\n",
        "                else:\n",
        "                    return self.predictor(x1)\n",
        "        \n",
        "        if verbose:\n",
        "            breaker()\n",
        "            print(\"Building Model ...\")\n",
        "            print(\"\\n{} -> 1\".format(IL))\n",
        "        \n",
        "        torch.manual_seed(seed)\n",
        "        model = ANN(IL=IL)\n",
        "        \n",
        "        return model\n",
        "\n",
        "\n",
        "    def fit(model=None, optimizer=None, scheduler=None, \n",
        "            epochs=None, early_stopping_patience=None,\n",
        "            dataloaders=None, fold=None, lr=None, wd=None, verbose=False) -> tuple:\n",
        "        \n",
        "        name = \"./Concat_Fold_{}_state.pt\".format(lr, wd, fold)\n",
        "        \n",
        "        if verbose:\n",
        "            breaker()\n",
        "            print(\"Training Fold {}...\".format(fold))\n",
        "            breaker()\n",
        "        else:\n",
        "            print(\"Training Fold {}...\".format(fold))\n",
        "\n",
        "        Losses = []\n",
        "        bestLoss = {\"train\" : np.inf, \"valid\" : np.inf}\n",
        "\n",
        "        start_time = time()\n",
        "        for e in range(epochs):\n",
        "            e_st = time()\n",
        "            epochLoss = {\"train\" : np.inf, \"valid\" : np.inf}\n",
        "\n",
        "            for phase in [\"train\", \"valid\"]:\n",
        "                if phase == \"train\":\n",
        "                    model.train()\n",
        "                else:\n",
        "                    model.eval()\n",
        "                \n",
        "                lossPerPass = []\n",
        "\n",
        "                for X, y in dataloaders[phase]:\n",
        "                    X, y = X.to(DEVICE), y.to(DEVICE)\n",
        "\n",
        "                    optimizer.zero_grad()\n",
        "                    with torch.set_grad_enabled(phase == \"train\"):\n",
        "                        output = model(X)\n",
        "                        loss = torch.nn.MSELoss()(output, y)\n",
        "                        if phase == \"train\":\n",
        "                            loss.backward()\n",
        "                            optimizer.step()\n",
        "                    lossPerPass.append(loss.item())\n",
        "                epochLoss[phase] = np.mean(np.array(lossPerPass))\n",
        "            Losses.append(epochLoss)\n",
        "\n",
        "            if early_stopping_patience:\n",
        "                if epochLoss[\"valid\"] < bestLoss[\"valid\"]:\n",
        "                    bestLoss = epochLoss\n",
        "                    BLE = e + 1\n",
        "                    torch.save({\"model_state_dict\": model.state_dict(),\n",
        "                                \"optim_state_dict\": optimizer.state_dict()},\n",
        "                            name)\n",
        "                    early_stopping_step = 0\n",
        "                else:\n",
        "                    early_stopping_step += 1\n",
        "                    if early_stopping_step > early_stopping_patience:\n",
        "                        if verbose:\n",
        "                            print(\"\\nEarly Stopping at Epoch {}\".format(e))\n",
        "                        break\n",
        "            \n",
        "            if epochLoss[\"valid\"] < bestLoss[\"valid\"]:\n",
        "                bestLoss = epochLoss\n",
        "                BLE = e + 1\n",
        "                torch.save({\"model_state_dict\": model.state_dict(),\n",
        "                            \"optim_state_dict\": optimizer.state_dict()},\n",
        "                        name)\n",
        "            \n",
        "            if scheduler:\n",
        "                scheduler.step(epochLoss[\"valid\"])\n",
        "            \n",
        "            if verbose:\n",
        "                print(\"Epoch: {} | Train Loss: {:.5f} | Valid Loss: {:.5f} | Time: {:.2f} seconds\".format(e+1, epochLoss[\"train\"], epochLoss[\"valid\"], time()-e_st))\n",
        "        \n",
        "        if verbose:\n",
        "            breaker()\n",
        "            print(\"Best Validation Loss at Epoch {}\".format(BLE))\n",
        "            breaker()\n",
        "            print(\"Time Taken [{} Epochs] : {:.2f} minutes\".format(len(Losses), (time()-start_time)/60))\n",
        "            breaker()\n",
        "            print(\"Training Completed\")\n",
        "            breaker()\n",
        "\n",
        "        return Losses, BLE, name\n",
        "\n",
        "    #####################################################################################################\n",
        "\n",
        "    def predict_batch(model=None, dataloader=None, mode=\"test\", path=None) -> np.ndarray:    \n",
        "        model.load_state_dict(torch.load(path, map_location=DEVICE)[\"model_state_dict\"])\n",
        "        model.to(DEVICE)\n",
        "        model.eval()\n",
        "\n",
        "        y_pred = torch.zeros(1, 1).to(DEVICE)\n",
        "        if re.match(r\"valid\", mode, re.IGNORECASE):\n",
        "            for X, _ in dataloader:\n",
        "                X = X.to(DEVICE)\n",
        "                with torch.no_grad():\n",
        "                    output = model(X)\n",
        "                y_pred = torch.cat((y_pred, output.view(-1, 1)), dim=0)\n",
        "        elif re.match(r\"test\", mode, re.IGNORECASE):\n",
        "            for X in dataloader:\n",
        "                X = X.to(DEVICE)\n",
        "                with torch.no_grad():\n",
        "                    output = model(X)\n",
        "                y_pred = torch.cat((y_pred, output.view(-1, 1)), dim=0)\n",
        "        \n",
        "        return y_pred[1:].detach().cpu().numpy()\n",
        "\n",
        "\n",
        "    def train(features: np.ndarray, targets: np.ndarray, n_splits: int, \n",
        "              batch_size: int, lr: float, wd: float, \n",
        "              epochs: int, early_stopping: int, \n",
        "              patience=None, eps=None):\n",
        "\n",
        "        metrics = []\n",
        "            \n",
        "        KFold_start_time = time()\n",
        "        if verbose:\n",
        "            breaker()\n",
        "            print(\"Performing {} Fold CV ...\".format(n_splits))\n",
        "        else:\n",
        "            breaker()\n",
        "        fold = 1\n",
        "        for tr_idx, va_idx in KFold(n_splits=n_splits, shuffle=True, random_state=SEED).split(features):\n",
        "\n",
        "            tr_features, va_features = features[tr_idx], features[va_idx]\n",
        "            tr_targets, va_targets = targets[tr_idx], targets[va_idx]\n",
        "\n",
        "            tr_targets = sc_y.fit_transform(tr_targets)\n",
        "            va_targets = sc_y.transform(va_targets)\n",
        "\n",
        "            dataloaders = build_dataloaders(tr_features, va_features,\n",
        "                                            tr_targets, va_targets, \n",
        "                                            batch_size, SEED)\n",
        "            model = build_model(IL=tr_features.shape[1], seed=SEED).to(DEVICE)\n",
        "            optimizer = model.get_optimizer(lr=lr, wd=wd)\n",
        "            scheduler = None\n",
        "            if isinstance(patience, int) and isinstance(eps, float):\n",
        "                scheduler = model.get_plateau_scheduler(optimizer, patience, eps)\n",
        "\n",
        "            L, _, name = fit(model=model, optimizer=optimizer, scheduler=scheduler, \n",
        "                            epochs=epochs, early_stopping_patience=early_stopping,\n",
        "                            dataloaders=dataloaders, fold=fold, lr=lr, wd=wd, verbose=verbose)\n",
        "            y_pred = predict_batch(model=model, dataloader=dataloaders[\"valid\"], mode=\"valid\", path=name)\n",
        "            RMSE = np.sqrt(mean_squared_error(sc_y.inverse_transform(y_pred), sc_y.inverse_transform(va_targets)))\n",
        "            if verbose:\n",
        "                print(\"Validation RMSE [Fold {}]: {:.5f}\".format(fold, RMSE))\n",
        "                breaker()\n",
        "                show_graphs(L)\n",
        "            \n",
        "            metrics_dict = {\"Fold\" : fold, \"RMSE\" : RMSE}\n",
        "            metrics.append(metrics_dict)\n",
        "            \n",
        "            fold += 1\n",
        "        \n",
        "        if verbose:\n",
        "            breaker()\n",
        "            print(\"Total Time to {} Fold CV : {:.2f} minutes\".format(n_splits, (time() - KFold_start_time)/60))\n",
        "        \n",
        "        return metrics, (time() - KFold_start_time)/60\n",
        "    \n",
        "    metrics, _ = train(features, targets, 10, 128, 1e-3, 1e-1, 100, 8)\n",
        "\n",
        "    if verbose:\n",
        "        pass\n",
        "    else:\n",
        "        breaker()\n",
        "    rmse = []\n",
        "    for i in range(len(metrics)):\n",
        "        print(\"Fold {}, RMSE {:.5f}\".format(metrics[i][\"Fold\"], metrics[i][\"RMSE\"]))\n",
        "        rmse.append(metrics[i][\"RMSE\"])\n",
        "    breaker()\n",
        "\n",
        "    best_index = rmse.index(min(rmse))\n",
        "\n",
        "    print(\"Best RMSE : {:.5f}\".format(metrics[best_index][\"RMSE\"]))\n",
        "    print(\"Avg RMSE  : {:.5f}\".format(sum(rmse) / len(rmse)))\n",
        "\n",
        "    with open(\"metrics_2.pkl\", \"wb\") as fp:\n",
        "        pickle.dump(metrics, fp)\n",
        "\n",
        "    breaker() "
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BaT9haNcYVTc"
      },
      "source": [
        "## Main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBcbb8lXYWZ6"
      },
      "source": [
        "def main():\n",
        "    features_1 = np.load(os.path.join(PATH_UA, \"densenet169_features.npy\"))\n",
        "    features_2 = np.load(os.path.join(PATH_A, names[0]))\n",
        "    features_3 = np.load(os.path.join(PATH_A, names[1]))\n",
        "    features_4 = np.load(os.path.join(PATH_A, names[2]))\n",
        "    features_5 = np.load(os.path.join(PATH_A, names[3]))\n",
        "    features_6 = np.load(os.path.join(PATH_A, names[4]))\n",
        "\n",
        "    features = np.concatenate((features_1, features_2, features_3, features_4, features_5, features_6), axis=0)\n",
        "    targets = get_targets()\n",
        "\n",
        "    breaker()\n",
        "    print(\"\\t --- INDEPENDENT ---\")\n",
        "\n",
        "    do_independent(features_1, features_2, features_3, features_4, features_5, features_6, targets)\n",
        "\n",
        "    print(\"\\t --- CONCAT ---\")\n",
        "    targets = np.array([targets for _ in range(6)]).reshape(-1).reshape(-1, 1)\n",
        "    do_concat(features, targets)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fzz4cM_ov1ao",
        "outputId": "57eca41e-5640-4287-e499-19ec397844f0"
      },
      "source": [
        "main()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "**************************************************\n",
            "\n",
            "\t --- INDEPENDENT ---\n",
            "\n",
            "**************************************************\n",
            "\n",
            "Training Fold 1...\n",
            "Training Fold 2...\n",
            "Training Fold 3...\n",
            "Training Fold 4...\n",
            "Training Fold 5...\n",
            "Training Fold 6...\n",
            "Training Fold 7...\n",
            "Training Fold 8...\n",
            "Training Fold 9...\n",
            "Training Fold 10...\n",
            "\n",
            "**************************************************\n",
            "\n",
            "Fold 1, RMSE 18.91639\n",
            "Fold 2, RMSE 18.79880\n",
            "Fold 3, RMSE 19.02069\n",
            "Fold 4, RMSE 18.14201\n",
            "Fold 5, RMSE 19.41679\n",
            "Fold 6, RMSE 19.05259\n",
            "Fold 7, RMSE 19.27267\n",
            "Fold 8, RMSE 17.07658\n",
            "Fold 9, RMSE 18.60525\n",
            "Fold 10, RMSE 18.47649\n",
            "\n",
            "**************************************************\n",
            "\n",
            "Best RMSE : 17.07658\n",
            "Avg RMSE  : 18.67783\n",
            "\n",
            "**************************************************\n",
            "\n",
            "\t --- CONCAT ---\n",
            "\n",
            "**************************************************\n",
            "\n",
            "Training Fold 1...\n",
            "Training Fold 2...\n",
            "Training Fold 3...\n",
            "Training Fold 4...\n",
            "Training Fold 5...\n",
            "Training Fold 6...\n",
            "Training Fold 7...\n",
            "Training Fold 8...\n",
            "Training Fold 9...\n",
            "Training Fold 10...\n",
            "\n",
            "**************************************************\n",
            "\n",
            "Fold 1, RMSE 19.54191\n",
            "Fold 2, RMSE 20.34399\n",
            "Fold 3, RMSE 19.48926\n",
            "Fold 4, RMSE 19.95675\n",
            "Fold 5, RMSE 19.72336\n",
            "Fold 6, RMSE 19.83964\n",
            "Fold 7, RMSE 20.05939\n",
            "Fold 8, RMSE 19.81487\n",
            "Fold 9, RMSE 19.57920\n",
            "Fold 10, RMSE 19.95267\n",
            "\n",
            "**************************************************\n",
            "\n",
            "Best RMSE : 19.48926\n",
            "Avg RMSE  : 19.83010\n",
            "\n",
            "**************************************************\n",
            "\n"
          ]
        }
      ]
    }
  ]
}